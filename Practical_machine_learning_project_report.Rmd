---
title: "Practical Machine Learning Project Report - Human Activity Recognition - Weight Lifting"
author: "Youyi Liu"
date: "8/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.Introduction
Our data in this case comes from the research on the human activity recognition. The purpose of this research is using recorded sensor data to predict the actual action of the users. The research is conducted by Wallace Ugulino, Eduardo Velloso and Hugo Fuks. The detail of this research could be reached at http://groupware.les.inf.puc-rio.br/har.

In this case, the project is using sensor data to predict the real action in a weight lifting scenario. The actions include:

```{r classetable, echo=FALSE, message=FALSE, warning=FALSE}

HARMATRIX <-      rbind(c("A", "Exactly according to the specification"), 
                        c("B", "Throwing the elbows to the front"), 
                        c("C", "lifting the dumbbell only halfway"),
                        c("D", "Lowering the dumbbell only halfway"),
                        c("E", "Throwing the hips to the front"))
colnames(HARMATRIX) <- c("classe", "Description")
HARMATRIX
```

The dataset record data from 4 different positions of a Weight lifting exercise: arm, forearm, belt(waist) and the dumbbell. 

# 2.Data Exploration & Preparation
First we load the data into our data base
```{r data preparation, echo=FALSE, message=FALSE, warning=FALSE}
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists("pml-training.csv")){
        download.file(trainurl, "pml-training.csv", method = "curl")
}
if(!file.exists("pml-testing.csv")){
        download.file(testurl, "pml-testing.csv", method = "curl")
}
trainset <- read.csv("pml-training.csv")
testset <- read.csv("pml-testing.csv")
dim(trainset)
dim(testset)
```

There are 160 columns of metrics and 19622 rows of observations in the training data set and the testset has the same columns of metrics but 20 observations. In other word, the trainset has 99.9% and the testset has 0.1% of the total data. So I consider the testset as the final test set and conducting cross validation in the trainset.


### 2.1 Refine Metrics
Before we dive deep into the dataset, we should have a better understand of the columns. The data has four main measurements, which are sensors located seperatedly: 

the glove(forearm):

```{r metrics, echo=FALSE, message=FALSE, warning=FALSE}
metrics <- colnames(trainset)
forearmcol <- metrics[grep("forearm", metrics)]
forearmcol
```
the arm-band(arm):

```{r arm-bandcol, echo=FALSE, message=FALSE, warning=FALSE}
armcol <- metrics[grep("arm", metrics)[grep("arm", metrics)!= grep("forearm", metrics)]]
armcol
```
the belt:

```{r beltcol, echo = FALSE, message= FALSE, warning = FALSE}
beltcol <- metrics[grep("belt", metrics)]
beltcol
```
the dumbbell:

```{r dumbbellcol, echo =FALSE, message=FALSE, warning=FALSE}
dumbbellcol <- metrics[grep("dumbbell", metrics)]
dumbbellcol
```
There are 38 columns each describing the data gathered from the 4 sensors. 160 - 38*4 = 8.

There are 8 columns left including: 
```{r restcol, echo=FALSE, message=FALSE, warning=FALSE}
metrics[c(1:7, 160)]
ignorecol <- metrics[c(1,3:7)]
```
Classe would be our prediction variable. 
```{r classe, echo=FALSE, message=FALSE, warning=FALSE}
classe <- trainset[160]
summary(trainset[160])
```

raw_timestamp_part1, raw_timestamp_part2 and cvtd_timestamp could be treated as one. 
```{r timestamps,echo=FALSE, message=FALSE, warning=FALSE}
summary(trainset[3:5])
```

X stands for the number of rows. 

new_window and num_window represent the if the time window was the first one and the series number of windows, and thus could be ignored:

```{r ignores, echo=FALSE, warning=FALSE, message=FALSE}
summary(trainset[c(1,6:7)])
```

Therefore, the predictors could be divided into 5 main groups: 

different user(user_name), 

glove sensor parameters ("forearm" group),

arm-band sensor parameters("arm" group), 

belt sensor parameters("belt" group), 

dumbbell sensor parameters("dumbbell" group). 

### 2.2 Data Cleaning

Then we need to dive into the Features and select the most useful predictors, to filter out useless variables.

In each group of sensor data, there are 38 variables included. 

The Euler Angles(roll, pitch and yaw) readings are recorded as same as 8 features(mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness). 3 + 3*8 = 27

The accelerometer reading of Cartesian coordinate readings(x,y,z) are recorded as same as 2 features(total and variance). 3 + 2 = 5

The gyroscope's and magnetometer's cartesion coordinate readings(x,y,z) 3

The magnetometer's cartesion coordinate readings(x,y,z) 3

The total is 27 + 5 + 3 + 3 = 38

Some of them may be highly related.

We look closely into the data in the forearm data as an example:

```{r forearmdata, echo=FALSE, message=FALSE, warning=FALSE}
forearmdata <- trainset[forearmcol]
```

```{r forearmcleanning, echo = FALSE, warning=FALSE}
facleancol <- c(4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21,22,23,24,25,26,27,28,29)
forearmclean <- forearmdata[facleancol]
summary(forearmclean)
c1colnames <- colnames(forearmclean)
forearmdata1 <- forearmdata[-facleancol]
```
There are 25 factor variables out of 38 in this dataset have little information. These variables have almost 98% none recorded data, NAs and only 2% of the information are recorded. Then these variables should be dropped from the candidates of variables before conducting a feature selection. The same cleanning strategies are applied to other groups of data. The cleaned dataset has the following structure:
```{r armdata, echo=FALSE, message=FALSE, warning=FALSE}
armdata <- trainset[armcol]
acleancol <- c(5,6,7,8,9,10,11,12,13,14,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38)
armdata1 <- armdata[-acleancol]
c2colnames <- colnames(armdata[acleancol])
```

```{r beltdata, echo=FALSE, message=FALSE, warning=FALSE}
beltdata <- trainset[beltcol]
bcleancol <- c(5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21:29)
beltdata1 <- beltdata[-bcleancol]
c3colnames <- colnames(beltdata[bcleancol])

```

```{r dumbbelldata, echo=FALSE, message=FALSE, warning=FALSE}
dumbbelldata <- trainset[dumbbellcol]
dcleancol <- c(4:18,20:29)
dumbbelldata1 <- dumbbelldata[-dcleancol]
c4colnames <- colnames(dumbbelldata[dcleancol])
```

```{r testdatacleaning, echo=FALSE, message=FALSE, warning=FALSE}
cleancolnames <- c(c1colnames,c2colnames,c3colnames,c4colnames,ignorecol)
training <- trainset[, !(colnames(trainset) %in% cleancolnames)]
testing <- testset[, !(colnames(testset) %in% cleancolnames)]
str(training)
```
After these cleaning have been done, we would also notice that the variables comes from the same group may be highly correlated, hence bias in the prediction. Therefore, we may consider doing the regularization regression in the model at this point.

### 2.3 Cross validation

In order to detect relevant parameters and leave the testset untouched before final test, we will go a cross validation process on our trainset. Using cross validation will help in avoiding over-ftting. k-fold cross validation was took into consideration. 

In particular, 10-fold cross validation was picked for use, making the training set splited into 9 testsets and 1 validation set for training 10 times, each fold was used for validation set once. 5-fold cross validation will be considered. Since increasing the folds of cross validation will increase the accuracy for sure but will also increase the variance. Therefore, to find the large enough fold number we try setting two sets of k-fold cross validation here.
                                        
```{r cross_validation, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
train_control10 <- trainControl(method = "CV", number = 10)
train_control5 <- trainControl(method = "CV", number = 5)
```

Running simple Decision Tree models will help us choose the k's value in cross validation

```{r trainingrpart, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(9527)
Rpartmodel10 <- train(classe ~., data = training, trControl = train_control10, method = "rpart")
Rpartmodel5 <- train(classe ~., data = training, trControl = train_control5, method = "rpart")
print(Rpartmodel5)
print(Rpartmodel10)
```

```{r rpartvis, echo=FALSE, message=FALSE, warning=FALSE}
library(rattle)
fancyRpartPlot(Rpartmodel5$finalModel)
```

We can see that from 5 to 10 folds, the Accuracy does not increase too much between the two models above. 5-fold cross validation is accurate enough in our modeling. Then we will use a 5-fold cross validation in the following modeling process. 

## 3. Training Models and Validations

To find the most fitted model, several models are used for exploration. To control bias in different variables, each model will have a preprocess that normalizing the variables. 

### 3.1 Gradient Boosting classification

```{r traininggbm, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(9527)
GBmodel <- train(classe ~., data = training, trControl = train_control5, preProcess = c("center", "scale"), method = "gbm", verbose = FALSE)
print(GBmodel)
```
The estimate of gradient boosting classification model's accuracy is 0.96.

### 3.2 Random Forest

```{r trainingrf, echo=FALSE, message=FALSE, warning=FALSE}
RFmodel <- train(classe ~., data = training, trControl = train_control5, preProcess = c("center", "scale"), method = "rf")
print(RFmodel)
```
The estimate of random forest model's accuracy is 0.99.

### 3.3 K-Nearest Neighbour
```{r trainingbg, echo=FALSE, message=FALSE, warning=FALSE}
KNNmodel <- train(classe ~., data = training, trControl = train_control5, preProcess = c("center", "scale"), method = "knn")
print(KNNmodel)
```
The estimate of k nearest neighbour model's accuracy is 0.97.

* Therefore, after comparison of three common machine learning models. random forest model will be used for the result's prediction.

## 4. Conclusion

The final result of our model is listed below:

```{r predict, echo=FALSE, message=FALSE, warning=FALSE}
Results <- predict(RFmodel, newdata = testing)
Results
```


